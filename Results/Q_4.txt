
  "took" : 120,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 18309,
    "max_score" : 674.4914,
    "hits" : [
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "10066",
        "_score" : 193.43462,
        "_source" : {
          "identifier" : "H2020ICT20161",
          "Text" : "European Cloud InMemory Database Appliance with Predictable Performance for Critical Applications The project aims at producing a European Cloud Database Appliance for providing a Database as a Service able to match the predictable performance robustness and trustworthiness of on premise architectures such as those based on mainframes The project will evolve cloud architectures to enable the increase of the uptake of cloud technology by providing the robustness trustworthiness and performance required for applications currently considered too critical to be deployed on existing cloudsCloudDBAppliance will deliver a cloud database appliance featuring1    A scalable operational database able to process high update workloads such as the ones processed by banks or telcos combined with a fast analytical engine able to answer analytical queries in an online manner 2    A Hadoop data lake integrated with the operational database to cover the needs from companies on big data3    A cloud hardware appliance leveraging the next generation of hardware to be produced by Bull the main European hardware provider This hardware is a scaleup hardware similar to the one of mainframes but with a more modern architecture Both the operational database and the inmemory analytics engine will be optimized to fully exploit this hardware and deliver predictable performance Additionally CloudDBAppliance will deal with the need to tolerate catastrophic cloud data centres failures eg a fire or natural disaster providing data redundancy across cloud data centres",
          "Rcn" : "206178",
          "Acronym" : "CloudDBAppliance"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "6022",
        "_score" : 172.24763,
        "_source" : {
          "identifier" : "H2020ICT2015",
          "Text" : """STREAMLINE STREAMLINE will address the competitive advantage needs of European online media businesses EOMB by delivering fast reactive analytics suitable in solving a wide array of problems including addressing customer retention personalised recommendation and more broadly targeted services STREAMLINE will develop crosssectorial analytics drawing on multisource data originating from online media consumption online games telecommunications services and multilingual web content STREAMLINE partners face big and fast data challenges They serve over 100 million users offer services that produce billions of events yielding over 10 TB of data daily and possess over a PB of data at rest Their business usecases are representative of EOMB which cannot be handled efficiently  effectively by stateoftheart technologies as a consequence of system and human latencies System latency issues arise due to the lack of appropriate data streamoriented analytics tools and more importantly the added complexity cost and burden associated with jointly supporting analytics for both data at rest and data in motion Human latency results from the heterogeneity of existing tools and the low level programming languages required for development using an inordinate number of boilerplate codes that are system specific eg Hadoop SolR Esper Storm and databases and a plethora of scripts required to glue systems togetherOur research and innovation actions include addressing the challenges brought on by system and human latencies In this regard STREAMLINE will1	Develop a high level declarative language and userinterface and corresponding automatic optimisation parallelisation and system adaptation technologies that reduce the programming expertise required by data scientists thereby enabling them to more freely focus on domain specific matters2	Overcome the complexity of the socalled lambda architecture by delivering simplified operations that jointly support data at rest and data in motion in a single system and is compatible with the Hadoop ecosystem3	Develop fast reactive machine learning technologies based on distributed parameter servers and fully distributed asynchronous and approximate algorithms for fast results at high input ratesThe impact of developing a European open source tool for analysing data at rest and data in motion in a single system featuring a high level declarative language and a fast reactive machine learning library is much wider than just the recommender ad targeting and customer retention applications that the industrial partners in STREAMLINE will use to demonstrate the business value of our work for the data economy Our open source tools will help Europe in general since they lower the big data analytics skills barrier broaden the reach of data analytics tools and are applicable to diverse market sectors including healthcare manufacturing and transportation Thereby enabling a broad number of European SMEs in other markets to explore and integrate these technologies into their businesses At the same time STREAMLINE will provide a solid foundation for big data leadership in Europe by providing an opensource platform ready to be used by millions of stakeholders in companies households and governmentThe STREAMLINE consortium comprises worldrenowned scientists and innovators in the areas of database systems DFKI distributed systems SICS and machine learning SZTAKI who have won many international awards hold 18 patents collectively and have founded and advised nine startups Complementing the research excellence are four leading European enterprises in the data economy in the areas of global telecommunication services eg Internet IPTV mobile and landline networks PT games and entertainment Rovio media content streaming NMusic and webscale data extraction and business analytics IMR with Petab""",
          "Rcn" : "199862",
          "Acronym" : "STREAMLINE"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "11313",
        "_score" : 170.63214,
        "_source" : {
          "identifier" : "ERC2016COG",
          "Text" : "The Computational Database for Real World Awareness Two major hardware trends have a significant impact on the architecture of database management systems DBMSs First main memory sizes continue to grow significantly Machines with 1TB of main memory and more are readily available at a relatively low price Second the number of cores in a system continues to grow from currently 64 and more to hundreds in the near future This trend offers radically new opportunities for both business and science  It promises to allow for informationatyourfingertips ie large volumes of data can be analyzed and deeply explored online in parallel to regular transaction processing Currently deep data exploration is performed outside of the database system which necessitates huge data transfers This impedes the processing such that realtime interactive exploration is impossible  These new hardware capabilities now allow to build a true computational database system that integrates deep exploration functionality at the source of the data This will lead to a drastic shift in how users interact with data as for the first time interactive data exploration becomes possible at a massive scaleUnfortunately traditional DBMSs are simply not capable to tackle these new challengesTraditional techniques like interpreted code execution for query processing become a severe bottleneck in the presence of such massive parallelism causing poor utilization of the hardware I pursue a radically different approach Instead of adapting the traditional diskbased approaches I am integrating a new justintime compilation framework into the inmemory database that directly exploits the abundant parallel hardware for largescale data processing and exploration By explicitly utilizing cores I will be able to build a powerful computational database engine that scales the entire spectrum of data processing  from transactional to analytical to exploration workflows  far beyond traditional architectures",
          "Rcn" : "207899",
          "Acronym" : "CompDB"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "11990",
        "_score" : 168.55107,
        "_source" : {
          "identifier" : "H2020MSCAIF2016",
          "Text" : "Next gEneration Sequence sTORage Sequential data are everywhere from DNA sequences to astronomical light curves and from aircraft engine monitoring data to the prices of stock options Recent advances in various fields such as those of data storage networking and sensing technologies have allowed organizations to gather overwhelming amounts of sequential data at unprecedented speedsThis wealth of information enables analysts to identify patterns find abnormalities and extract knowledge It is noteworthy that common practice in various domains is to use custom data analysis solutions usually built using higher level programming languages such as RPython Such techniques however while commonly acceptable in small data processing scenarios are unfit for larger scale data management and exploration This is because they come in contrast to all previous database research not taking advantage of indexes physical data independence query optimization and data processing methods designed for scalability In these domains database systems are used merely for storing and retrieving data and not as the sophisticated query processing systems they areCurrent relational storage layers cannot handle the access patterns that analysts of sequential data are interested in without scanning large amounts of unnecessary data or without large processing overhead Thus making complex analytics inefficientIn order to exploit this new opportunity we plan to develop specialized data series storage and retrieval systems which will allow analysts  across different fields  to efficiently manipulate the sequences of interestThe proposed research project named NESTOR Next gEneration Sequence sTORage has the potential of great economic and social impact in Europe as multiple scientific and industrial fields are currently in need of the right tools in order to handle their massive collections of data series",
          "Rcn" : "208656",
          "Acronym" : "NESTOR"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "5505",
        "_score" : 159.8706,
        "_source" : {
          "identifier" : "H2020EO2015",
          "Text" : "Platform for wildlife monitoring integrating Copernicus and ARGOS data EO4wildlife main objective is to bring large number of multidisciplinary scientists such as biologists ecologists and ornithologists around the world to collaborate closely together while using European Sentinel Copernicus Earth Observation more heavily and efficientlyIn order to reach such important objective an open service platform and interoperable toolbox will be designed and developed It will offer high level services that can be accessed by scientists to perform their respective research The platform front end will be easytouse access and offer dedicated services that will enable them process their geospatial environmental stimulations using Sentinel Earth Observation data that are intelligently combined with other observation sourcesSpecifically the EO4wildlife platform will enable the integration of Sentinel data ARGOS archive databases and real time thematic databank portals including Wildlifetrackingorg Seabirdtrackingorg and other Earth Observation and MetOcean databases locally or remotely and simultaneouslyEO4wildlife research specialises in the intelligent management big data processing advanced analytics and a Knowledge Base for wildlife migratory behaviour and trends forecast The research will lead to the development of webenabled open services using OGC standards for sensor observation and measurements and data processing of heterogeneous geospatial observation data and uncertaintiesEO4wildlife will design implement and validate various scenarios based on real operational use case requirements in the field of wildlife migrations habitats and behaviour These include 1 Management tools for regulatory authorities to achieve realtime advanced decisionmaking on the protection of protect seabird species 2 Enhancing scientific knowledge of pelagic fish migrations routes reproduction and feeding behaviours for better species management and 3 Setting up tools to assist marine protected areas and management",
          "Rcn" : "199237",
          "Acronym" : "EO4wildlife"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "2893",
        "_score" : 158.60713,
        "_source" : {
          "identifier" : "H2020MSCAIF2014",
          "Text" : "A Scalable and Elastic Platform for NearRealtime Analytics for The Graph of Everything The SMARTER A Scalable and Elastic Platform for NearRealtime Analytics for The Graph of EveryThing project aims to build a platform that provide the ability to derive actionable information from enormous amount of data generated by the Internet of Everything  to leverage datadriven strategies to innovate compete and capture value from deep web and realtime information The project targets innovative research outcomes by addressing Big Dynamic Data Analytic requirements from three relevant aspects variety and velocity and volume The project introduces the concept Graph of Everything GoT  to deal with the issue of data variety in data analytics for Internet of Things IoT data The Graph of Everything extends Linked Data model RDF  that has been widely used for representing deep web data to connect dynamic data from data streams generated from IoT eg sensor readings with any knowledgebase to create a single graph as an integrated database serving any analytical queries on a set of nodesedges of the graph so called analytical lens of everything The dynamic data represented as Linked Data Model called Linked Stream Data may contain valuable but perishable insights which are only valuable if it can be detected to act on them right at the right time Moreover to derive such insights the dynamic data needs to be correlated with various large datasets Therefore SMARTER has to deal both the velocity  requirements together volume requirements of analysing GoT to make the platform able support nearrealtime analytical operations with the elastically scalability",
          "Rcn" : "196064",
          "Acronym" : "SMARTER"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "10948",
        "_score" : 157.01505,
        "_source" : {
          "identifier" : "ERC2016STG",
          "Text" : "A Rigorous Approach to Consistency in Cloud Databases Modern Internet services store data in novel cloud databases which partition and replicate the data across a large number of machines and a wide geographical span To achieve high availability and scalability cloud databases need to maximise the parallelism of data processing Unfortunately this leads them to weaken the guarantees they provide about data consistency to applications The resulting programming models are very challenging to use correctly and we currently do not have advanced methods and tools that would help programmers in this taskThe goal of the project is to develop a synergy of novel reasoning methods static analysis tools and database implementation techniques that maximally exploit parallelism inside cloud databases while enabling application programmers to ensure correctness We intend to achieve this by first developing methods for reasoning formally about how weakening the consistency guarantees provided by cloud databases affects application correctness and the parallelism allowed inside the databases This will build on techniques from the areas of programming languages and software verification The resulting theory will then serve as a basis for practical implementation techniques and tools that harness database parallelism but only to the extent such that its side effects do not compromise application correctnessThe proposed project is highrisk because it aims not only to develop a rigorous theory of consistency in cloud databases but also to apply it to practical systems design The project is also highgain since it will push the envelope in availability scalability and costeffectiveness of cloud databases",
          "Rcn" : "207381",
          "Acronym" : "RACCOON"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "1093",
        "_score" : 151.9157,
        "_source" : {
          "identifier" : "H2020ICT20141",
          "Text" : "AEGLE Ancient Greek    An analytics framework for integrated and personalized healthcare services in Europe The data generated in the health domain is coming from heterogeneous multimodal multilingual dynamic and fast evolving medical technologies Today we are found in a big health landscape characterized by large volume versatility and velocity 3Vs which has led to the evolution of the informatics in the big biodata domain AEGLE project will build an innovative ICT solution addressing the whole data value chain for health based on cloud computing enabling dynamic resource allocation HPC infrastructures for computational acceleration and advanced visualization techniques AEGLE will Realize a multiparametric platform using algorithms for analysing big biodata including features such as volume properties communication metrics and bottlenecks estimation of related computational resources needed handling data versatility and managing velocity  Address the systemic health big biodata in terms of the 3V multidimensional space using analytics based on PCA techniques  Demonstrate AEGLEs efficiency through the provision of aggregated services covering the 3V space of big biodata Specifically it will be evaluated in abig biostreams where the decision speed is critical and needs nonlinear and multiparametric estimators for clinical decision support within limited time bbigdata from nonmalignant diseases where the need for NGS and molecular data analytics requires the combination of cloud located resources coupled with local demands for data and visualization and finally cbigdata from chronic diseases including EHRs and medication with needs for quantified estimates of important clinical parameters semantics extraction and regulatory issues for integrated care  Bring together all related stakeholders leading to integration with existing open databases increasing the speed of AEGLE adaptation  Build a business ecosystem for the wider exploitation and targeting on crossborder production of custom multilingual solutions based on AEGLE",
          "Rcn" : "194261",
          "Acronym" : "AEGLE"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "6806",
        "_score" : 151.14406,
        "_source" : {
          "identifier" : "ERC2015CoG",
          "Text" : "Translating from Multiple Modalities into Text Recent years have witnessed the development of a wide range of computational methods that process and generate natural language text  Many of these have become familiar to mainstream computer users such as tools that retrieve documents matching a query perform sentiment analysis and translate between languages Systems like Google Translate can instantly translate between any pair of over fifty human languages allowing users to read web content that wouldnt have otherwise been available The accessibility of the web could be further enhanced with applications that translate within the same language between different modalities or different data formats  There are currently no standard tools for simplifying language eg for lowliteracy readers or second language learners The web is rife with nonlinguistic data eg databases images source code that cannot be searched since most retrieval tools operate over textual data In this project we maintain that in order to render electronic data more accessible to individuals and computers alike new types of models need to be developed Our proposal is to provide a unified framework for translating from comparable corpora ie collections consisting of data in the same or different modalities that address the same topic without being direct translations of each other We will develop general and scalable models that can solve different translation tasks and learn the necessary intermediate representations of the units involved in an unsupervised manner without extensive feature engineering Thanks to recent advances in deep  learning we will induce  representations for different modalities their interactions and correspondence to natural language Beyond addressing a fundamental aspect of the translation problem the proposed research will lead to novel internetbased applications that simplify and summarize text produce documentation for source code and meaningful descriptions for images",
          "Rcn" : "200779",
          "Acronym" : "TransModal"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "7681",
        "_score" : 143.18083,
        "_source" : {
          "identifier" : "ERC2015CoG",
          "Text" : "A programming language bridging theory and practice for scientific data curation Science is increasingly datadriven Scientific research funders now routinely mandate open publication of publiclyfunded research data Safely reusing such data currently requires labourintensive curation Provenance recording the history and derivation of the data is critical to reaping the benefits and avoiding the pitfalls of data sharing There are hundreds of curated scientific databases in biomedicine that need finegrained provenance one important example is GtoPdb a pharmacological database developed by colleagues in Edinburgh Currently there are no reusable methodologies or practical tools that support provenance for curated databases forcing each project to start from scratch Research on provenance for scientific databases is still at an early stage and prototypes have so far proven challenging to deploy or evaluate in the field Also most techniques to date focus on provenance within a single database but this is only part of the problem real solutions will have to integrate database provenance with the multiple tiers of web applications and noone has begun to address this challengeI propose research on how to build support for curation into the programming language itself building on my recent research on the Links Web programming language and on data curation Links is a stronglytyped language that provides stateoftheart support for languageintegrated query and Web programming I propose to build on Links and other recent language designs for heterogeneous metaprogramming to develop a new language called Skye that can express modular reusable curation and provenance techniques To keep focus on the real needs of scientific databases Skye will be evaluated in the context of GtoPdb and other scientific database projects Bridging the gap between curation research and the practices of scientific database curators will catalyse a virtuous cycle that will increase the pace of breakthrough results from datadriven science",
          "Rcn" : "202602",
          "Acronym" : "Skye"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "4735",
        "_score" : 142.7914,
        "_source" : {
          "identifier" : "H2020EINFRA20151",
          "Text" : "Open Digital Research Environment Toolkit for the Advancement of Mathematics OpenDreamKit will deliver a flexible toolkit enabling research groups to set up Virtual Research Environments customised to meet the varied needs of research projects in pure mathematics and applications and supporting the full research lifecycle from exploration through proof and publication to archival and sharing of data and codeOpenDreamKit will be built out of a sustainable ecosystem of communitydeveloped open software databases and services including popular tools such as LinBox MPIR Sagesagemathorg GAP PariGP LMFDB and Singular We will extend the Jupyter Notebook environment to provide a flexible UI By improving and unifying existing building blocks OpenDreamKit will maximise both sustainability and impact with beneficiaries extending to scientific computing physics chemistry biology and more and including researchers teachers and industrial practitioners We will define a novel componentbased VRE architecture and the adapt existing mathematical software databases and UI components to work well within it on varied platforms  Interfaces to standard HPC and grid services will be built in  Our architecture will be informed by recent research into the sociology of mathematical collaboration so as to properly support actual research practice The ease of set up adaptability and global impact will be demonstrated in a variety of demonstrator VREsWe will ourselves study the social challenges associated with largescale open source code development and of publications based on executable documents to ensure sustainabilityOpenDreamKit will be conducted by a Europewide demandsteered collaboration including leading mathematicians computational researchers and software developers long track record of delivering innovative open source software solutions for their respective communities All produced code and tools will be open source",
          "Rcn" : "198334",
          "Acronym" : "OpenDreamKit"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "10070",
        "_score" : 142.41925,
        "_source" : {
          "identifier" : "H2020ICT20161",
          "Text" : "DeveloperCentric Knowledge Mining from Large OpenSource Software Repositories Recent reports state that the adoption of opensource software OSS helps resulting in savings of about 60 billion per year to consumers However the use of OSS also comes at enormous cost choosing among OSS projects and maintaining dependence on continuously changing software requires a large investment Deciding if an OSS project meets the required standards for adoption is hard and keeping uptodate with an evolving project is even harder It involves analysing code documentation online discussions and issue trackers There is too much information to process manually and it is common that uninformed decisions have to be made with detrimental effectsCROSSMINER remedies this by automatically extracting the required knowledge and injecting it into the IDE of the developers at the time they need it to make their design decisions This allows them to reduce their effort in knowledge acquisition and to increase the quality of their code CROSSMINER uniquely combines advanced software project analyses with online monitoring in the IDE The developer will be monitored to infer which information is timely based on readily available knowledge stored earlier by a set of advanced offline deep analyses of related OSS projects To achieve this timely and ambitious goal CROSSMINER combines six enduser partners in the domains of IoT multisector IT services API coevolution software analytics software quality assurance and OSS forges along with RD partners that have a long trackrecord in conducting cuttingedge research on largescale software analytics natural language processing reverse engineering of software components modeldriven engineering and delivering results in the form of widelyused sustainable and industrialstrength OSS The development of the CROSSMINER platform is guided by an advisory board of worldclass experts and the dissemination of the project will be led by The Open Group",
          "Rcn" : "206182",
          "Acronym" : "CROSSMINER"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "5999",
        "_score" : 142.40155,
        "_source" : {
          "identifier" : "H2020ICT2015",
          "Text" : "Scalable online machine learning for predictive analytics and realtime interactive visualization PROTEUS mission is to investigate and develop readytouse scalable online machine learning algorithms and interactive visualization techniques for realtime predictive analytics to deal with extremely large data sets and data streams The developed algorithms and techniques will form a library to be integrated into an enhanced version of Apache Flink the EU Big Data platform PROTEUS will contribute to the EU Big Data area by addressing fundamental challenges related to the scalability and responsiveness of analytics capabilities The requirements are defined by a steelmaking industrial use case The techniques developed in PROTEUS are however general flexible and portable to all data streambased domains In particular the project will go beyond the current stateofart technology by making the following specific original contributionsi Realtime scalable machine learning for massive highvelocity and complex data streams analyticsii Realtime hybrid computation batch data and data streamsiii Realtime interactive visual analytics for Big Dataiv Enhancement of Apache Flink the EU Big Data platform andv Realworld industrial validation of the technology developedThe PROTEUS impact is manifold i strategic by reducing the gap and dependency from the US technology empowering the EU Big Data industry through the enrichment of the EU platform Apache Flink ii economic by fostering the development of new skills and new job positions and opportunities towards economic growth iii industrial by considering realworld requirements from industry and by validating the outcome on an operational setting and iv scientific by developing original hybrid and streaming analytic architectures that enable scalable online machine learning strategies and advanced interactive visualisation techniques that are applicable for general data streams in other domains",
          "Rcn" : "199839",
          "Acronym" : "PROTEUS"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "13768",
        "_score" : 140.52184,
        "_source" : {
          "identifier" : "H2020MSCAIF2016",
          "Text" : "Biomedical Information Synthesis with Deep Natural Language Inference Deep neural networks DNNs have become a critical tool in natural language processing NLP for a wide variety of language technologies from syntax to semantics to pragmatics In particular in the field of natural language inference NLI DNNs have become the defacto model providing significantly better results than previous paradigms Their power lies in their ability to embed complex language ambiguities in high dimensional spaces coupled with nonlinear compositional transformations learned to directly optimize taskspecific objective functions We propose to adapt Deep NLI techniques to the biomedical domain specifically investigating question answering information extraction and synthesis The biomedical domain presents many key challenges and a critical impact that standard NLI challenges do not posses First while standard NLI data sets requires a system to model basic world knowledge eg that soccer is a sport they do not presume a rich domain knowledge encoded in various and often heterogeneous resources such as scientific articles textbooks and structured databases Second while standard NLI data sets presume that the answerinference is encoded in a single utterance the ability to reason and extract information from biomedical domains often requires information synthesis from multiple utterances paragraphs and even documents Finally whereas standard NLI is a broad challenge aimed at testing whether computers can make general inferences in language biomedical texts are a grounded and impactful domain where progress in automated reasoning will directly impact the efficacy of researchers physicians publishers and policy makers",
          "Rcn" : "210588",
          "Acronym" : "DNLIBiomed"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "1079",
        "_score" : 139.89624,
        "_source" : {
          "identifier" : "H2020ICT20141",
          "Text" : "A Holistic Data Privacy and Security by Design PlatformasaService Framework Introducing Distributed Encrypted Persistence in Cloudbased Applications The vision of PaaSword is to maximize and fortify the trust of individual professional and corporate customers to Cloud enabled services and applications to safeguard both corporate and personal sensitive data stored on Cloud infrastructures and Cloudbased storage services and to accelerate the adoption of Cloud computing technologies and paradigm shift from the European industry Thus PaaSword will introduce a holistic data privacy and security by design framework enhanced by sophisticated contextaware policy access models and robust policy access decision enforcement and governance mechanisms which will enable the implementation of secure and transparent Cloudbased applications and services that will maintain a fully distributed and totally encrypted data persistence layer and thus will foster customers data protection integrity and confidentiality even in the case wherein there is no control over the underlying thirdparty Cloud resources utilizedIn particular PaaSword intends not only to adopt the CSA Cloud security principles but also to extend them by capitalizing on recent innovations on a distributed encryption and virtual database middleware technologies that introduce a scalable secure Cloud database abstraction layer combined with sophisticated distribution and encryption methods into the processing and querying of data stored in the Cloud b contextaware access control that incorporate the dynamically changing contextual information into novel group policies implementing configurable contextbased access control policies and contextdependent access rights to the stored data at various different levels and c policy governance modelling and annotation techniques that allows application developers to specify an appropriate level of protection for the applications data while the evaluation of whether an incoming request should be granted access to the target data takes dynamically place during application runtime",
          "Rcn" : "194247",
          "Acronym" : "PaaSword"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "6292",
        "_score" : 137.44354,
        "_source" : {
          "identifier" : "ERC2015CoG",
          "Text" : "Foundations of Factorized Data Management Systems The objective of this project is to investigate scalability questions arising with a new wave of smart relational data management systems that integrate analytics and query processing These questions will be addressed by a fundamental shift from centralized processing on tabular data representation as supported by traditional systems and analytics software packages to distributed and approximate processing on factorized data representationFactorized representations exploit algebraic properties of relational algebra and the structure of queries and analytics to achieve radically better data compression than generic compression schemes while at the same time allowing processing in the compressed domain They can effectively boost the performance of relational processing by avoiding redundant computation in the oneserver setting yet they can also be naturally exploited for approximate and distributed processing Large relations can be approximated by their subsets and supersets ie lower and upper bounds that factorize much better than the relations themselves Factorizing relations which represent intermediate results shuffled between servers in distributed processing can effectively reduce the communication cost and improve the latency of the systemThe key deliverables will be novel algorithms that combine distribution approximation and factorization for computing mixed loads of queries and predictive and descriptive analytics on largescale data This research will result in fundamental theoretical contributions such as complexity results for largescale processing and tractable algorithms and also in a scalable factorized data management system that will exploit these theoretical insights We will collaborate with industrial partners who are committed to assist in providing datasets and realistic workloads infrastructure for largescale distributed systems and support for transferring the products of the research to industrial users",
          "Rcn" : "200139",
          "Acronym" : "FADAMS"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "15504",
        "_score" : 136.34966,
        "_source" : {
          "identifier" : "H2020MSCAIF2016",
          "Text" : "INtelligeNt ApplicatiOns oVer Large ScAle DaTa StrEams Large scale data analytics is the key research domain for future data driven applications as numerous of devices produce huge volumes of data in the form of streams Analytics services can offer the necessary basis for building intelligent decision making mechanisms to support novel applications Due to the huge volumes of data analytics should be based on efficient schemes for querying large scale data partitions Partitions contain only a piece of data and a dedicated processor manages the incoming queries The management of continuous queries over data streams is a challenging research issue requiring intelligent methods to derive the final outcome ie query response in limited time with maximum performance The management process of continuous queries involves their assignment to specific processors and the processing of the derived responses We focus on a group of query controllers serving the incoming queries and thus becoming the connection of big data systems with the real world INNOVATE proposes solutions for the management of the controllers behavior We propose an intelligent decision making process for each controller in three axes i topdown by realizing a mechanism that assigns queries to the underlying processors ii bottomup by proposing decision making mechanisms for returning responses to usersapplications on top of early results iii horizontal by proposing optimization schemes for queries management We adopt a pool of learning schemes and an ensemble learning model dealing with how and on which processors each query should be assigned We also propose specific schemes for combining processors responses Intelligent and optimization techniques are adopted for the controllers group management Machine learning Computational Intelligence and optimization are the key adopted technologies that when combined provide efficient solutions to a challenging problem like the support of intelligent analytics over big data streams",
          "Rcn" : "212617",
          "Acronym" : "INNOVATE"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "6582",
        "_score" : 135.37086,
        "_source" : {
          "identifier" : "H2020MSCAITN2015",
          "Text" : "Methodologies and Data mining techniques for the analysis of Big Data based on Longitudinal Population and Epidemiological Registers European societies face rapid social changes challenges and benefits which can be studied with traditional tools of analysis but with serious limitations This rapid transformation covers changes in family forms fertility the decline of mortality and increase of longevity and periods of economic and social instability Owing to population ageing across Europe countries are now the experiencing the impact of these rapid changes on the sustainability of their welfare systems At the same time the use of the space and residential mobility has become a key topic with migrations within the EU countries and from outside Europe being at the center of the political agenda Over the past decade research teams across Europe have been involved in the development and construction of longitudinal population registers and large research databases while opening up avenues for new linkages between different data sources ie administrative and health data making possible to gain an understanding of these fast societal transformations However in order to work with these types of datasets requires advanced skills in both data management and statistical techniques LONGPOP aims to create network to utilize these different research teams to share experiences construct joint research create a training track for specialist in the field and increase the number of users of these large  possibly underused  databases making more scientists and stakeholders aware of the richness in the databases",
          "Rcn" : "200475",
          "Acronym" : "LONGPOP"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "16475",
        "_score" : 135.29176,
        "_source" : {
          "identifier" : "H2020SMEINST120162017",
          "Text" : "An ICT decision support system for clinical interpretation of genomic data used by geneticists to rapidly and accurately pinpoint unique diseasecausing variants With the increasing volume of genetic sequences bioinformatics methods data about variants and mutations and research publications clinical interpretation remains a complex and labourintensive task As a result clinical interpretation became the costly piece of genetic testing limiting scale keeping turnaround time high and driving up costs of a test Moreover after the geneticists work is done clinicians still remain with partial insufficient information to base their clinical decisions onEmedgene have developed an ICT platform for clinical interpretation of genomic data The platform continuously scans all public resources and databases available retrieves information from written publications using Natural Language Processing and intelligently integrates the data into unified ontologies It is the first artificial intelligence platform modelled on the behaviour of sophisticated genomic interpreters to automatically pinpoint the unique diseasecausing variants and deliver results with solid evidence and reasoning providing a clear path to clinical decisionsWithin the overall project Emedgene plan to optimise the platforms interpretations capabilities demonstrate and validate it in clinical environments and fully establish the European commercialisation strategy",
          "Rcn" : "213722",
          "Acronym" : "Emedgene"
        }
      },
      {
        "_index" : "movies",
        "_type" : "movie",
        "_id" : "14263",
        "_score" : 134.95258,
        "_source" : {
          "identifier" : "H2020JTIIMI2201507twostage",
          "Text" : """Big Data  Heart Despite remarkable progress in the management of cardiovascular disease CVD major unmet needs remain with regard to mortality hospitalisations quality of life QoL healthcare expenditures and productivity Acute coronary syndrome ACS atrial fibrillation AF and heart failure HF are major and growing components of the global CVD burden Optimal management of these conditions is complicated by their complex aetiology and heterogeneous prognoses Poor definition at the molecular level and co  multimorbidities form major challenges for the development and delivery of targeted treatments This renders response to therapy unpredictable with large interindividual variation and importantly small or undetectable treatment effects in large trials of unselected patientsTodays treatment guidelines still reflect the scientific constraints of an earlier era where clinical markers to guide therapy are limited to conventional risk factors and endorgan damage and where the main endpoint in clinical trials is patient death Hence drug development pipelines from early target validation through to late postmarketing work have proven to be slow expensive and highrisk the chance of eventual approval for a CVD drug candidate in Phase I trials is 7 the lowest of any disease category shared with oncology2 Moreover tolerability of medication and adherence to treatment show wide variations There is thus a need for better definition of these diseases their markers and endpoints including better segmentation of current heterogeneous patient groups acknowledging underlying mechanisms and comorbidities and of their outcomesprognoses including functional capacity and quality of life QoLBigDataHearts ultimate goal is to develop a Big Datadriven translational research platform of unparalleled scale and phenotypic resolution in order to deliver clinically relevant disease phenotypes scalable insights from realworld evidence and insights driving drug development and personalised medicine through advanced analytics  To accomplish this BigDataHeart will	Assemble an unparalleled array of bigdata sets	Create a responsive and agile research framework to address the objectives	Create systems to leverage existing stateoftheart solutions from different sources to enable combination for highpower identification harmonisation access and analysis of distributed data	Develop and expand on this distributed data for further analysis	Create novel frameworks for disease definitions based on uptodate scientific evidence	Leverage unique database access and big data science expertise to validate and tweak the novel frameworks through a number of pilot studies with high and immediate social relevance	Involve all relevant stakeholders including policy makers and insurers to ensure full support for this new framework	Ensure proper wide dissemination of the framework and project results to maximise impact and speed at which the results will be broadly implemented in the EU and so that they may serve as a template or inspiration for the rest of the world	Set new and durable standards for cardiovascular bigdata science for the next decadesBigDataHeart uniquely brings together key players and stakeholders in the CVD field to address the challenges outlined above The clinical researchers involved in BigDataHeart have been instrumental in shaping current treatment and management of HF AF and ACS They will join forces with leading epidemiologists and big data scientists from across Europe and leading European cardiovascular professional and patient organisations This team is complemented with a powerful group of players from the pharmaceutical industry Through its partners BigDataHeart has access to most of the relevant largescale European CVD databases ranging from electronic health records and disease registries through wellphenotyped clinical trials and large e""",
          "Rcn" : "211210",
          "Acronym" : "BigData@Heart"
        }
      }
    ]
  }
